{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neonatal Sepsis Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/Neonatal_Sepsis_Registry.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_intresting_parameters = [\"gestational_age_at_birth_weeks\",\n",
    "                              \"sex\",\n",
    "                              \"race\",\n",
    "                              \"birth_weight_kg\",\n",
    "                              \"sepsis_group\",\n",
    "                              \"onset_age_in_days\",\n",
    "                              \"onset_hour_of_day\",\n",
    "                            #   \"time_to_antibiotics\",\n",
    "                              \"stat_abx\",\n",
    "                              \"intubated_at_time_of_sepsis_evaluation\",\n",
    "                              \"inotrope_at_time_of_sepsis_eval\",\n",
    "                              \"central_venous_line\",\n",
    "                              \"umbilical_arterial_line\",\n",
    "                              \"ecmo\",\n",
    "                              \"temp_celsius\",\n",
    "                              \"comorbidity_necrotizing_enterocolitis\",\n",
    "                              \"comorbidity_chronic_lung_disease\",\n",
    "                              \"comorbidity_cardiac\",\n",
    "                              \"comorbidity_surgical\",\n",
    "                              \"comorbidity_ivh_or_shunt\"]\n",
    "\n",
    "df = df[list_intresting_parameters]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df,\n",
    "            y=\"gestational_age_at_birth_weeks\",\n",
    "            x=\"sepsis_group\",\n",
    "            hue=\"temp_celsius\",\n",
    "            palette=\"Spectral\",\n",
    "            row=\"sex\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelle ausprobieren\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data set in without na and male/female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('NI', np.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benutzerdefinierte Funktion zur Zuordnung von Werten zu 0 oder 1\n",
    "def map_to_binary(value):\n",
    "    if value == 1 or value in [4, 5, 6]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Eine neue Spalte \"sepsis_binary\" erstellen, indem Sie die benutzerdefinierte Funktion auf die \"sepsis_group\"-Spalte anwenden\n",
    "df['sepsis_binary'] = df['sepsis_group'].apply(map_to_binary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male = df[df[\"sex\"] == 1]\n",
    "df_female = df[df[\"sex\"] == 0]\n",
    "\n",
    "df_without_nan = df.dropna()\n",
    "df_without_nan[\"race\"] = df_without_nan[\"race\"].astype(int)\n",
    "df_male_without_nan = df_male.dropna()\n",
    "df_female_without_nan = df_female.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_nan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df_without_nan, columns=['race'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df_without_nan, x =\"sepsis_binary\", kind=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All gender with na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[df.columns.difference(['sepsis_group', \"sepsis_binary\"])]\n",
    "# df['sepsis_binary'] = df['sepsis_binary']\n",
    "# y = df['sepsis_binary']\n",
    "\n",
    "# # Train-Test-Split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# model = HistGradientBoostingClassifier()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = model.predict(X_test_scaled)\n",
    "# # accuracy = accuracy_score(y_test, y_pred)\n",
    "# # print(f\"{name}: {accuracy}\")\n",
    "# print(len(X_test), len(y_test), len(y_pred), y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All gender without na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded[df_encoded.columns.difference(['sepsis_binary', 'sepsis_group'])]\n",
    "df_encoded['sepsis_binary'] = df_encoded['sepsis_binary'] \n",
    "y = df_encoded['sepsis_binary']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Feature Scaling\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Schritt 3: Feature Selection mit SelectKBest auf den resamplten Trainingsdaten\n",
    "selector = SelectKBest(mutual_info_classif, k=3)  # Wählen Sie die besten 5 Merkmale aus\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train_resampled)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# [('learning_rate', 0.2), ('max_depth', 4), ('n_estimators', 150)]\n",
    "# Modelle initialisieren\n",
    "# criterion': 'entropy', 'max_depth': 30, 'max_features': 'auto', 'n_estimators': 200\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    # \"XGBoost\": XGBClassifier(eval_metric='mlogloss', learning_rate=0.2, max_depth=4, n_estimators=150),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss'),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"RF\": RandomForestClassifier(max_depth =30, min_samples_split= 2, n_estimators=200, criterion='entropy'),\n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"NB\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Modelle trainieren und evaluieren\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_selected, y_train_resampled.values.ravel())\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name}: {accuracy}\")\n",
    "\n",
    "# Nachdem Sie SelectKBest angewendet haben, können Sie die ausgewählten Indizes der Merkmale abrufen.\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Verwenden Sie die Indizes, um die Namen der ausgewählten Merkmale aus Ihrem ursprünglichen DataFrame abzurufen.\n",
    "selected_feature_names = X.columns[selected_feature_indices]\n",
    "\n",
    "# Drucken Sie die Namen der ausgewählten Merkmale aus.\n",
    "print(\"Ausgewählte Merkmale:\")\n",
    "print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Modelle, die du kombinieren möchtest\n",
    "models = [\n",
    "    ('LR', LogisticRegression()),\n",
    "    ('RF', RandomForestClassifier()),\n",
    "    ('XGB', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'))\n",
    "]\n",
    "\n",
    "# Voting Classifier erstellen\n",
    "voting_clf = VotingClassifier(estimators=models, voting='hard')\n",
    "\n",
    "# Training des Voting Classifiers\n",
    "voting_clf.fit(X_train_selected, y_train_resampled.values.ravel())\n",
    "\n",
    "# Evaluierung des Voting Classifiers\n",
    "y_pred = voting_clf.predict(X_test_selected)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Classifier: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_male_without_nan[df_male_without_nan.columns.difference(['sepsis_binary', 'sepsis_group'])]\n",
    "df_male_without_nan['sepsis_binary'] = df_male_without_nan['sepsis_binary'] \n",
    "y = df_male_without_nan['sepsis_binary']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modelle initialisieren\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"NB\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Modelle trainieren und evaluieren\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train_resampled.values.ravel())\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name}: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pca = PCA(n_components=5)  # Zum Beispiel, um die Dimensionalität auf 2 zu reduzieren\n",
    "X_pca = pca.fit_transform(df_without_nan)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(cumulative_variance)\n",
    "\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), cumulative_variance, marker='o', linestyle='-')\n",
    "plt.xlabel('Anzahl der Hauptkomponenten')\n",
    "plt.ylabel('Kumulative erklärte Varianz')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Ladevektoren für die ersten beiden Hauptkomponenten abrufen\n",
    "first_pc_loading_vector = pca.components_[0]\n",
    "second_pc_loading_vector = pca.components_[1]\n",
    "\n",
    "# Die Ladevektoren ausgeben\n",
    "print(\"Ladevektor für die erste Hauptkomponente:\")\n",
    "print(first_pc_loading_vector)\n",
    "\n",
    "print(\"\\nLadevektor für die zweite Hauptkomponente:\")\n",
    "print(second_pc_loading_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index der Spalte mit dem größten Beitrag zur ersten Hauptkomponente finden\n",
    "max_contributor_index = np.argmax(np.abs(first_pc_loading_vector))\n",
    "\n",
    "# Den Namen der Spalte aus dem DataFrame abrufen\n",
    "column_name = df_without_nan.columns[max_contributor_index]\n",
    "\n",
    "print(f\"Die Spalte mit dem größten Beitrag zur ersten Hauptkomponente ist '{column_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index der Spalte mit dem größten Beitrag zur ersten Hauptkomponente finden\n",
    "max_contributor_index = np.argmax(np.abs(second_pc_loading_vector))\n",
    "\n",
    "# Den Namen der Spalte aus dem DataFrame abrufen\n",
    "column_name = df_without_nan.columns[max_contributor_index]\n",
    "\n",
    "print(f\"Die Spalte mit dem größten Beitrag zur zweiten Hauptkomponente ist '{column_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_without_nan[df_without_nan.columns.difference(['sepsis_binary', 'sepsis_group'])]\n",
    "df_without_nan['sepsis_binary'] = df_without_nan['sepsis_binary'] \n",
    "y = df_without_nan['sepsis_binary']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Schritt 3: Feature Selection mit SelectKBest auf den resamplten Trainingsdaten\n",
    "selector = SelectKBest(mutual_info_classif, k=5)  # Wählen Sie die besten 5 Merkmale aus\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train_resampled)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "# Modelle initialisieren\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"NB\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Modelle trainieren und evaluieren\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_selected, y_train_resampled.values.ravel())\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name}: {accuracy}\")\n",
    "\n",
    "# Nachdem Sie SelectKBest angewendet haben, können Sie die ausgewählten Indizes der Merkmale abrufen.\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Verwenden Sie die Indizes, um die Namen der ausgewählten Merkmale aus Ihrem ursprünglichen DataFrame abzurufen.\n",
    "selected_feature_names = X.columns[selected_feature_indices]\n",
    "\n",
    "# Drucken Sie die Namen der ausgewählten Merkmale aus.\n",
    "print(\"Ausgewählte Merkmale:\")\n",
    "print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "xgb_params = {\n",
    "    'learning_rate': Real(0.01, 0.2, 'log-uniform'),\n",
    "    'max_depth': Integer(3, 10),\n",
    "    'n_estimators': Integer(50, 150)\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier( eval_metric='mlogloss', random_state=42)\n",
    "xgb_bayes_search = BayesSearchCV(xgb_model, xgb_params, n_iter=32, cv=5, n_jobs=-1, random_state=42)\n",
    "xgb_bayes_search.fit(X_train_selected, y_train_resampled.values.ravel())\n",
    "\n",
    "print('Beste Parameter für XGBoost:', xgb_bayes_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': Integer(50, 150),\n",
    "    'max_depth': Integer(10, 30),\n",
    "    'min_samples_split': Integer(2, 10)\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_bayes_search = BayesSearchCV(rf_model, rf_params, n_iter=32, cv=5, n_jobs=-1, random_state=42)\n",
    "rf_bayes_search.fit(X_train_selected, y_train_resampled.values.ravel())\n",
    "\n",
    "print('Beste Parameter für RF:', rf_bayes_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train_selected, y_train_resampled.values.ravel())\n",
    "\n",
    "# Beste Parameter ausgeben\n",
    "print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "\n",
    "# Beste Modell verwenden\n",
    "best_rf_model = grid_search.best_estimator_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
